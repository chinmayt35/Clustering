{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "import itertools\n",
    "\n",
    "import scipy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install pandas-profiling\n",
    "import pandas_profiling\n",
    "#from yellowbrick.cluster import SilhouetteVisualizer, InterclusterDistance, KElbowVisualizer\n",
    "\n",
    "#from kmodes.kmodes import KModes\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# This will ensure that matplotlib figures don't get cut off when saving with savefig()\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data and Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv files\n",
    "points_data = pd.read_csv(\"Points.csv\")\n",
    "customer_data = pd.read_csv(\"CustomerDetail.csv\")\n",
    "fact_attribute_data = pd.read_csv(\"FactAttribute.csv\")\n",
    "customer_extension_data = pd.read_csv(\"CustomerExtension.csv\")\n",
    "quality_activity_data = pd.read_csv(\"QualityActivity.csv\")\n",
    "statistic_data=pd.read_csv(\"PointTypeStatistics.csv\")\n",
    "cineplex_data=pd.read_csv(\"OnlineStore.csv\")\n",
    "account_history=pd.read_csv(\"AccountHistory.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Filtering and Aggregating data - Points Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering for Earned & Burned\n",
    "points_earned = points_data[(points_data.points >= 0)]\n",
    "points_burned = points_data[(points_data.points < 0)]\n",
    "\n",
    "#Aggregating points and transactions by customer id\n",
    "earned_agg = points_earned.groupby('Unique_member_identifier').agg(points_earned=pd.NamedAgg('points',sum))\n",
    "burned_agg = points_burned.groupby('Unique_member_identifier').agg(points_burned=pd.NamedAgg('points',sum))\n",
    "trans_agg = points_data.groupby('Unique_member_identifier').agg(trans_data=pd.NamedAgg('TransAmount',sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Creating features to capture time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "points_data['date'] = pd.to_datetime(points_data['pointdt'])\n",
    "points_data.info()\n",
    "points_data['hour']=points_data['date'].dt.hour\n",
    "\n",
    "conditions = [points_data['hour'].between(5, 11), points_data['hour'].between(11, 17), points_data['hour'].between(17, 21),points_data['hour'].between(21, 24),points_data['hour'].between(0, 5)]\n",
    "choices = ['morning','afternoon','evening','night','night']\n",
    "points_data['time'] = np.select(conditions, choices)\n",
    "\n",
    "#Filtering data by time\n",
    "morn = points_data[(points_data.time =='morning')]\n",
    "aft = points_data[(points_data.time =='afternoon')]\n",
    "eve = points_data[(points_data.time =='evening')]\n",
    "nig = points_data[(points_data.time =='night')]\n",
    "\n",
    "#Counts for day/aft/eve/night transactions\n",
    "morn_counts=morn.groupby('Unique_member_identifier').size().reset_index(name='mor_counts')\n",
    "aft_counts=aft.groupby('Unique_member_identifier').size().reset_index(name='aft_counts')\n",
    "eve_counts=eve.groupby('Unique_member_identifier').size().reset_index(name='eve_counts')\n",
    "nig_counts=nig.groupby('Unique_member_identifier').size().reset_index(name='nig_counts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 -  Creating Features using Quality Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove columns we don't need\n",
    "quality_activity_data = quality_activity_data.drop(['ActivityMonth','isReachable','isSMS','hasActivity'],axis=1)\n",
    "\n",
    "#Dummy variables\n",
    "quality_activity_data = pd.get_dummies(quality_activity_data, columns=(['isQuality','isMarketable']), drop_first=False)\n",
    "\n",
    "#Aggregate the number of trues and falses\n",
    "quality_activity_data = quality_activity_data.groupby('Unique_member_identifier').agg('sum')\n",
    "\n",
    "#lambda function to check which factor level has more occurences\n",
    "quality_activity_data['Quality'] = quality_activity_data.apply(lambda X: 'False' if X.isQuality_False > X.isQuality_True else 'True',axis = 1)\n",
    "quality_activity_data['Marketable'] = quality_activity_data.apply(lambda X: 'False' if X.isMarketable_False > X.isMarketable_True else 'True',axis = 1)\n",
    "\n",
    "#Remove dummy columns before merging\n",
    "quality_activity_data = quality_activity_data.drop(['isQuality_False','isMarketable_False','isQuality_True','isMarketable_True'],axis=1)\n",
    "quality_activity_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Special Card Activity Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dummy Variable for special card\n",
    "special_card = [11,12,1252,1253,1254,1282,1283,1290,1322,1323]\n",
    "special_card_activity=points_data.copy()\n",
    "special_card_activity.loc[special_card_activity['pointtypeid'].isin(special_card), 'isspecial_card'] = 1\n",
    "special_card_activity.loc[~special_card_activity['pointtypeid'].isin(special_card), 'isspecial_card'] = 0\n",
    "special_card_final=special_card_activity[['Unique_member_identifier','isspecial_card']]\n",
    "\n",
    "#Aggregating the count\n",
    "special_card_final=special_card_final.groupby('Unique_member_identifier').agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Applogon - Mobile/web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#App Login Codes\n",
    "app_logon_id = [606,607]\n",
    "app_logon_activity=account_history.copy()\n",
    "\n",
    "#Creating dummy's for app login\n",
    "app_logon_activity.loc[app_logon_activity['AccountHistoryTypeID'].isin(app_logon_id), 'app_web_logon'] = 1\n",
    "app_logon_activity.loc[~app_logon_activity['AccountHistoryTypeID'].isin(app_logon_id), 'app_web_logon'] = 0\n",
    "app_logon_final=app_logon_activity[['Unique_member_identifier','app_web_logon']]\n",
    "\n",
    "#Aggregating the counts for app login\n",
    "app_logon_final=app_logon_final.groupby('Unique_member_identifier').agg('sum')\n",
    "app_logon_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Deleting customers with no activity and merging all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting customers with no activity\n",
    "points_unique = pd.DataFrame(points_data['Unique_member_identifier'].unique())\n",
    "points_unique.rename(columns = {0:'Unique_member_identifier'},inplace=True)\n",
    "merged_data = pd.merge(customer_data,points_unique, on='Unique_member_identifier', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining features into customer detail dataframe\n",
    "merged_data = pd.merge(merged_data,earned_agg, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,burned_agg, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,trans_agg, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,morn_counts, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,aft_counts, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,eve_counts, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,nig_counts, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,quality_activity_data, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,special_card_final, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,app_logon_final, on='Unique_member_identifier', how='left')\n",
    "merged_data=merged_data.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.shape\n",
    "merged_data.info()\n",
    "merged_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Different Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Combining Fact Attribute, Customer Extension table with Customer Detail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_data,fact_attribute_data, on='Unique_member_identifier', how='left')\n",
    "merged_data = pd.merge(merged_data,customer_extension_data, on='Unique_member_identifier', how='left')\n",
    "merged_data = merged_data.drop(['isActive','FSA','CreateDt','ReferredBy'], axis=1)\n",
    "merged_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Combining Type Statistic Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data = pd.merge(merged_data,statistic_data, on='Unique_member_identifier', how='left')\n",
    "merged_data = merged_data.drop(['BlackEarnCount','BlackBurnCount','Unique_member_identifier','BlackEarnLastDt','BlackEarnPointTotal','BlackBurnLastDt','BlackBurnPointTotal','LoadTime','OrderCount','OrderPointTotal','OrderLastDt','ConcessionLastDt','MusicStoreLastDt','BlackActivityDays','CnplxOnlineBonusLastDt','CnplxEarnTuesdayLastDt','LastDt','ChildTicketLastDt'], axis=1)\n",
    "merged_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#FINAL DATSET FOR MODEL BUILDING\n",
    "merged_data = merged_data.fillna(0)\n",
    "df = merged_data.copy()\n",
    "df.head(3)\n",
    "df.info()\n",
    "df.to_csv('check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchial Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = df.shape[1]\n",
    "\n",
    "#Separating categorical and numerical variables\n",
    "cat_col_names = list(df.select_dtypes(include=np.object).columns)\n",
    "bool_col_names = list(df.select_dtypes(include=np.bool).columns)\n",
    "num_col_names = list(df.select_dtypes(include=np.number).columns)\n",
    "\n",
    "cat_col_names = cat_col_names + bool_col_names\n",
    "X_num = df[num_col_names].to_numpy()\n",
    "X_cat = df[cat_col_names].to_numpy()\n",
    "\n",
    "#Scaling only the numerical variables\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(X_num)\n",
    "\n",
    "X = np.concatenate((X_num, X_cat), axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a custom distance function for the heirarchial model to deal with categorical and numerical data\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def custom_dist(x1, x2, cat_cols=None, num_cols=None):\n",
    "   \n",
    "    n = len(x1)\n",
    "\n",
    "    dist_num = distance.euclidean(x1[num_cols], x2[num_cols])\n",
    "    dist_cat = distance.hamming(x1[cat_cols], x2[cat_cols])\n",
    "    \n",
    "    dist = (dist_num*sum(num_cols) + dist_cat*sum(cat_cols)) / n\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a custom distance matrix across all columns\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "cat_cols = [type(x)==str for x in X[1,:]]\n",
    "num_cols = [not x for x in cat_cols]\n",
    "\n",
    "Y = pdist(X, custom_dist, cat_cols=cat_cols, num_cols=num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Hierarchial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform hierarchical clustering, using our custom-built distance matrix.\n",
    "import scipy.cluster\n",
    "aggl = scipy.cluster.hierarchy.linkage(Y, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Dendogram\n",
    "plt.figure(figsize=(16, 10));\n",
    "plt.grid(False)\n",
    "dend = scipy.cluster.hierarchy.dendrogram(aggl);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using k as 8 (as per business requirement)\n",
    "K=8\n",
    "labels = scipy.cluster.hierarchy.fcluster(aggl, K, criterion=\"maxclust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Viewing all Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Viewing each cluster and its statistics\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "print('All Data:')\n",
    "print('Number of Instances: {}'.format(X.shape[0]))\n",
    "df.describe(include=[np.number]).transpose()\n",
    "df.describe(include=[np.object]).transpose()\n",
    "\n",
    "for col in cat_col_names:\n",
    "    df[col].value_counts()\n",
    "\n",
    "for i, label in enumerate(set(labels)):\n",
    "    n = df.iloc[labels==label].shape[0]\n",
    "          \n",
    "    print('\\nCluster {}:'.format(label))\n",
    "    print('Number of Instances: {}'.format(n))\n",
    "\n",
    "    df.iloc[labels==label].describe(include=[np.number]).transpose()\n",
    "    df.iloc[labels==label].describe(include=[np.object]).transpose()\n",
    "    \n",
    "    for col in cat_col_names:\n",
    "        df.iloc[labels==label][col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving numerical features into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = df.iloc[labels==1].shape[0]\n",
    "c1=df.iloc[labels==1].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster1.csv')\n",
    "\n",
    "n = df.iloc[labels==2].shape[0]\n",
    "c1=df.iloc[labels==2].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster2.csv')\n",
    "\n",
    "n = df.iloc[labels==3].shape[0]\n",
    "c1=df.iloc[labels==3].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster3.csv')\n",
    "\n",
    "n = df.iloc[labels==4].shape[0]\n",
    "c1=df.iloc[labels==4].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster4.csv')\n",
    "\n",
    "n = df.iloc[labels==5].shape[0]\n",
    "c1=df.iloc[labels==5].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster5.csv')\n",
    "\n",
    "\n",
    "n = df.iloc[labels==6].shape[0]\n",
    "c1=df.iloc[labels==6].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster6.csv')\n",
    "\n",
    "\n",
    "n = df.iloc[labels==7].shape[0]\n",
    "c1=df.iloc[labels==7].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster7.csv')\n",
    "\n",
    "\n",
    "n = df.iloc[labels==8].shape[0]\n",
    "c1=df.iloc[labels==8].describe(include=[np.number]).transpose()\n",
    "c1.to_csv('cluster8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session(\"result_final.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session(\"result_final.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
